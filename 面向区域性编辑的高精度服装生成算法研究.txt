面向区域性编辑的高精度服装生成算法研究

课题简介	随着生成式人工智能技术的发展，图像级服装编辑任务在虚拟试衣、时尚推荐、个性化广告等领域展现出巨大潜力。传统服装图像生成模型大多采用整体替换方式，缺乏对局部区域的精细控制，容易造成生成内容穿模、边界不清晰等问题，难以满足用户对 “精确编辑、自然融合” 的个性化需求。
本课题旨在研究一种面向区域性控制的高精度服装图像生成算法，聚焦在保持人物背景与非目标区域不变的前提下，对指定服装区域（如上衣、裤子、鞋子等）进行高保真替换。学生将探索图像语义分割、条件控制生成（如 Diffusion 或 GAN）、文本引导融合等前沿技术，设计一个可控性强、生成效果自然且风格多样的图像生成方法。重点不在于系统搭建，而在于算法创新与效果验证。
数据来源及规模	- DeepFashion2 数据集：包含 80 万 + 服装图像，附带关键点、区域遮罩与属性标注，可用于训练局部控制生成模型。
- SCHP 等人体解析数据集：提供语义分割标签，支持服装区域识别与编辑掩膜生成。
- 学生可自行搜索合适的数据
建议使用的数据规模为数万张训练图像，并结合数千组图文配对或目标区域标注。
目标要求	1. 生成算法实现：选用 Diffusion、GAN 或 ControlNet 等框架，构建条件生成模型；支持 mask 控制 + prompt 驱动的图像服装编辑；输出图像应保持原图背景、人物结构不变，变换部分细节清晰自然；可控的服装生成。
2. 文本引导控制机制（可选）：设计图文联合控制的提示机制，如 “将红色 T 恤替换为西装”；实现风格可调、内容可控的生成输出；
3. 效果评估与分析：使用 FID、LPIPS、Mask IOU、精度 (如 mIoU≥0.85) 等指标评估图像质量与局部控制准确性；提供丰富前后对比图，展示算法性能与适应能力。
能力要求	1. 熟悉 Diffusion 模型、GAN 或图像翻译类生成算法；
2. 掌握 PyTorch 深度学习框架，有训练模型的能力；
3. 了解图像分割或人体解析模型的使用与微调；
4. 具备基本的图像处理与评估能力；
报告撰写要求	1. 介绍任务背景、国内外相关研究及当前难点；
2. 描述所设计算法结构、各模块设计、训练流程与数据处理方法；
3. 给出详细实验结果，包括多个场景下的图像编辑效果对比；
4. 使用定量指标与定性分析结合的方式，对结果进行完整评估；
5. 总结模型优缺点，提出未来优化方向；
评分标准	1. 要求能够准确识别图像中待编辑的服装区域，并生成高质量的控制遮罩（mask）。评价指标包括 mIoU 等作为量化标准，须达到 mIoU ≥ 0.85 等
2. 要求实现完整且结构合理的区域控制生成模型，能够有效结合图像、遮罩和提示词输入，生成高保真图像。
3. 较强的创新性模型方法
4. 要求提交的报告结构清晰，内容完整，包含问题分析、方法设计、实验过程、效果评估等部分。需提供丰富的可视化结果与代码说明，能够反映项目的技术深度和实践能力。
